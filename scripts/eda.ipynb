{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32a2ad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "573b9e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 159,571\n",
      "Test samples: 63,978\n",
      "Train columns: ['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "Test columns: ['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "test_data = pd.read_csv('../data/test_1.csv')\n",
    "\n",
    "print(f\"Train samples: {len(train_data):,}\")\n",
    "print(f\"Test samples: {len(test_data):,}\")\n",
    "print(f\"Train columns: {list(train_data.columns)}\")\n",
    "print(f\"Test columns: {list(test_data.columns)}\")\n",
    "\n",
    "labels = list(train_data.columns)\n",
    "y_labels = labels[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4433755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "id               0\n",
      "comment_text     0\n",
      "toxic            0\n",
      "severe_toxic     0\n",
      "obscene          0\n",
      "threat           0\n",
      "insult           0\n",
      "identity_hate    0\n",
      "dtype: int64\n",
      "Test data:\n",
      "id               0\n",
      "comment_text     0\n",
      "toxic            0\n",
      "severe_toxic     0\n",
      "obscene          0\n",
      "threat           0\n",
      "insult           0\n",
      "identity_hate    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# missing values\n",
    "print(\"Train data:\")\n",
    "print(train_data.isnull().sum())\n",
    "print(\"Test data:\")\n",
    "print(test_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d21e3f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-toxic comment:\n",
      "Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n",
      "\n",
      "Toxic comment:\n",
      "Labels: toxic, severe_toxic, obscene, insult\n",
      "COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\n"
     ]
    }
   ],
   "source": [
    "# example comments\n",
    "print(\"Non-toxic comment:\")\n",
    "non_toxic = train_data[train_data['toxic']==0].iloc[0]\n",
    "print(f\"{non_toxic['comment_text']}\")\n",
    "print()\n",
    "print(\"Toxic comment:\")\n",
    "toxic = train_data[train_data['toxic']==1].iloc[0]\n",
    "print(f\"Labels: {', '.join([col for col in labels if toxic[col] == 1])}\")\n",
    "print(f\"{toxic['comment_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f96944f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of positive samples per label:\n",
      "toxic         :  15294 (9.58%)\n",
      "severe_toxic  :   1595 (1.00%)\n",
      "obscene       :   8449 (5.29%)\n",
      "threat        :    478 (0.30%)\n",
      "insult        :   7877 (4.94%)\n",
      "identity_hate :   1405 (0.88%)\n",
      "\n",
      "Percentage of comments with n labels:\n",
      "0 labels: 143346 (89.83%)\n",
      "1 labels:   6360 ( 3.99%)\n",
      "2 labels:   3480 ( 2.18%)\n",
      "3 labels:   4209 ( 2.64%)\n",
      "4 labels:   1760 ( 1.10%)\n",
      "5 labels:    385 ( 0.24%)\n",
      "6 labels:     31 ( 0.02%)\n"
     ]
    }
   ],
   "source": [
    "# label distribution\n",
    "print(\"Percentage of positive samples per label:\")\n",
    "for col in y_labels:\n",
    "    count = train_data[col].sum()\n",
    "    pct = (count / len(train_data)) * 100\n",
    "    print(f\"{col:14s}: {count:6d} ({pct:.2f}%)\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(f\"Percentage of comments with n labels:\")\n",
    "train_data['num_labels'] = train_data[y_labels].sum(axis=1)\n",
    "for i in range(7):\n",
    "    count = (train_data['num_labels'] == i).sum()\n",
    "    pct = (count / len(train_data)) * 100\n",
    "    print(f\"{i} labels: {count:6d} ({pct:5.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ef2588b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count:\n",
      "count    159571.000000\n",
      "mean         67.273527\n",
      "std          99.230702\n",
      "min           1.000000\n",
      "25%          17.000000\n",
      "50%          36.000000\n",
      "75%          75.000000\n",
      "max        1411.000000\n",
      "Name: word_count, dtype: float64\n",
      "\n",
      "Avg length of toxic comments: 51.3\n",
      "Avg length of non-toxic comments: 69.0\n"
     ]
    }
   ],
   "source": [
    "# text length \n",
    "train_data['word_count'] = train_data['comment_text'].str.split().str.len()\n",
    "\n",
    "print(\"Word count:\")\n",
    "print(train_data['word_count'].describe())\n",
    "print()\n",
    "print(f\"Avg length of toxic comments: {train_data[train_data['toxic']==1]['word_count'].mean():.1f}\")\n",
    "print(f\"Avg length of non-toxic comments: {train_data[train_data['toxic']==0]['word_count'].mean():.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c336a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
      "toxic          1.000         0.309    0.677   0.157   0.648          0.266\n",
      "severe_toxic   0.309         1.000    0.403   0.124   0.376          0.202\n",
      "obscene        0.677         0.403    1.000   0.141   0.741          0.287\n",
      "threat         0.157         0.124    0.141   1.000   0.150          0.115\n",
      "insult         0.648         0.376    0.741   0.150   1.000          0.338\n",
      "identity_hate  0.266         0.202    0.287   0.115   0.338          1.000\n"
     ]
    }
   ],
   "source": [
    "# Label correlation\n",
    "correlation = train_data[y_labels].corr()\n",
    "print(correlation.round(3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs178",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
